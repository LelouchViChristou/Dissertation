{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "      <th>medoids</th>\n",
       "      <th>cluster_sizes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hep-ph/0610334</td>\n",
       "      <td>[[-0.09863124, -0.025400802, -0.005175168, 0.0...</td>\n",
       "      <td>[[-0.113838255, -0.013086513, -0.026049882, 0....</td>\n",
       "      <td>[55, 19, 55, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2104.06416</td>\n",
       "      <td>[[-0.084981225, -0.08507558, 0.03543399, 0.086...</td>\n",
       "      <td>[[-0.13890694, -0.045757502, 0.0331088, 0.0221...</td>\n",
       "      <td>[61, 49, 40, 28]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hep-ph/9606269</td>\n",
       "      <td>[[-0.116921924, -0.031099621, 0.09050446, 0.07...</td>\n",
       "      <td>[[-0.09846101, 0.05293004, 0.047359765, -0.025...</td>\n",
       "      <td>[28, 19, 12, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hep-ph/9811382</td>\n",
       "      <td>[[-0.05011667, -0.0072394763, -0.017491272, 0....</td>\n",
       "      <td>[[-0.10917934, -0.025503034, -0.004675309, 0.0...</td>\n",
       "      <td>[46, 33, 10, 23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1304.2781</td>\n",
       "      <td>[[-0.05021094, -0.04983033, -0.02687403, -0.02...</td>\n",
       "      <td>[[-0.054514293, -0.08432221, -0.044620816, -0....</td>\n",
       "      <td>[7, 7, 8, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25166</th>\n",
       "      <td>909992</td>\n",
       "      <td>[[-0.07872735, -0.009727927, 0.023001013, -0.0...</td>\n",
       "      <td>[[0.020977847, 0.00994313, 0.016100913, -0.020...</td>\n",
       "      <td>[3, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25167</th>\n",
       "      <td>910046</td>\n",
       "      <td>[[0.0024761495, 0.029174268, -0.121854655, 0.0...</td>\n",
       "      <td>[[-0.04889003, -0.027657501, -0.03703226, 0.00...</td>\n",
       "      <td>[3, 1, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25168</th>\n",
       "      <td>910075</td>\n",
       "      <td>[[-0.03798147, 0.0035953722, 0.03408878, 0.035...</td>\n",
       "      <td>[[0.011452884, 0.14479369, -0.02908832, 0.0719...</td>\n",
       "      <td>[3, 2, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25169</th>\n",
       "      <td>910092</td>\n",
       "      <td>[[-0.022506248, -0.034485348, -0.053791, 0.072...</td>\n",
       "      <td>[[-0.022506248, -0.034485348, -0.053791, 0.072...</td>\n",
       "      <td>[2, 1, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25170</th>\n",
       "      <td>910312</td>\n",
       "      <td>[[0.02741555, 0.07259655, -0.020830669, 0.0688...</td>\n",
       "      <td>[[-0.021110417, 0.13329503, 0.0009666801, -0.0...</td>\n",
       "      <td>[10, 13, 6, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25171 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                          embedding  \\\n",
       "0      hep-ph/0610334  [[-0.09863124, -0.025400802, -0.005175168, 0.0...   \n",
       "1          2104.06416  [[-0.084981225, -0.08507558, 0.03543399, 0.086...   \n",
       "2      hep-ph/9606269  [[-0.116921924, -0.031099621, 0.09050446, 0.07...   \n",
       "3      hep-ph/9811382  [[-0.05011667, -0.0072394763, -0.017491272, 0....   \n",
       "4           1304.2781  [[-0.05021094, -0.04983033, -0.02687403, -0.02...   \n",
       "...               ...                                                ...   \n",
       "25166          909992  [[-0.07872735, -0.009727927, 0.023001013, -0.0...   \n",
       "25167          910046  [[0.0024761495, 0.029174268, -0.121854655, 0.0...   \n",
       "25168          910075  [[-0.03798147, 0.0035953722, 0.03408878, 0.035...   \n",
       "25169          910092  [[-0.022506248, -0.034485348, -0.053791, 0.072...   \n",
       "25170          910312  [[0.02741555, 0.07259655, -0.020830669, 0.0688...   \n",
       "\n",
       "                                                 medoids     cluster_sizes  \n",
       "0      [[-0.113838255, -0.013086513, -0.026049882, 0....   [55, 19, 55, 8]  \n",
       "1      [[-0.13890694, -0.045757502, 0.0331088, 0.0221...  [61, 49, 40, 28]  \n",
       "2      [[-0.09846101, 0.05293004, 0.047359765, -0.025...   [28, 19, 12, 7]  \n",
       "3      [[-0.10917934, -0.025503034, -0.004675309, 0.0...  [46, 33, 10, 23]  \n",
       "4      [[-0.054514293, -0.08432221, -0.044620816, -0....      [7, 7, 8, 3]  \n",
       "...                                                  ...               ...  \n",
       "25166  [[0.020977847, 0.00994313, 0.016100913, -0.020...      [3, 1, 1, 1]  \n",
       "25167  [[-0.04889003, -0.027657501, -0.03703226, 0.00...      [3, 1, 2, 1]  \n",
       "25168  [[0.011452884, 0.14479369, -0.02908832, 0.0719...      [3, 2, 1, 1]  \n",
       "25169  [[-0.022506248, -0.034485348, -0.053791, 0.072...      [2, 1, 2, 1]  \n",
       "25170  [[-0.021110417, 0.13329503, 0.0009666801, -0.0...    [10, 13, 6, 6]  \n",
       "\n",
       "[25171 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "\n",
    "# Read the dataframe\n",
    "df1 = pd.read_pickle('augmented_data_10k.pkl')\n",
    "df2 = pd.read_pickle('augmented_data.pkl')\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1536,)\n",
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert medoids to list of numpy arrays\n",
    "medoids_list = df['medoids'].apply(lambda x: np.array(x).reshape(-1))\n",
    "\n",
    "# Stack them into a single numpy array and convert to PyTorch tensor\n",
    "medoids_np = np.stack(medoids_list.to_numpy())\n",
    "print(medoids_np[0].shape)\n",
    "medoids_tensor = torch.FloatTensor(medoids_np)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(medoids_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1536])\n"
     ]
    }
   ],
   "source": [
    "from model import SimpleAutoencoder\n",
    "# Model Initialization\n",
    "model = SimpleAutoencoder().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "print(next(iter(dataloader))[0].shape)\n",
    "\n",
    "# from model import CNN_Autoencoder\n",
    "# # Model Initialization\n",
    "# model = CNN_Autoencoder().to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.MSELoss()\n",
    "# print(next(iter(dataloader))[0].shape)\n",
    "\n",
    "\n",
    "# from model import RecurrentAutoencoder\n",
    "# # Load the trained model\n",
    "# model = RecurrentAutoencoder()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.MSELoss()\n",
    "# print(next(iter(dataloader))[0].shape)\n",
    "\n",
    "# from model import TransformerAutoencoder\n",
    "# # Example usage\n",
    "# embed_dim = 384  # Example embedding dimension\n",
    "# num_heads = 4    # Example number of heads in multi-head attention\n",
    "# dim_feedforward = 1024  # Example feedforward dimension\n",
    "# num_layers = 2  # Example number of layers in the transformer encoder\n",
    "# seq_length = 4  # Original sequence length\n",
    "\n",
    "# model = TransformerAutoencoder(embed_dim, num_heads, dim_feedforward, num_layers, seq_length).to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.MSELoss()\n",
    "# print(next(iter(dataloader))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.0024\n",
      "Epoch [2/200], Loss: 0.0023\n",
      "Epoch [3/200], Loss: 0.0022\n",
      "Epoch [4/200], Loss: 0.0022\n",
      "Epoch [5/200], Loss: 0.0021\n",
      "Epoch [6/200], Loss: 0.0021\n",
      "Epoch [7/200], Loss: 0.0021\n",
      "Epoch [8/200], Loss: 0.0021\n",
      "Epoch [9/200], Loss: 0.0020\n",
      "Epoch [10/200], Loss: 0.0020\n",
      "Epoch [11/200], Loss: 0.0019\n",
      "Epoch [12/200], Loss: 0.0019\n",
      "Epoch [13/200], Loss: 0.0019\n",
      "Epoch [14/200], Loss: 0.0019\n",
      "Epoch [15/200], Loss: 0.0019\n",
      "Epoch [16/200], Loss: 0.0019\n",
      "Epoch [17/200], Loss: 0.0019\n",
      "Epoch [18/200], Loss: 0.0019\n",
      "Epoch [19/200], Loss: 0.0019\n",
      "Epoch [20/200], Loss: 0.0018\n",
      "Epoch [21/200], Loss: 0.0018\n",
      "Epoch [22/200], Loss: 0.0018\n",
      "Epoch [23/200], Loss: 0.0018\n",
      "Epoch [24/200], Loss: 0.0018\n",
      "Epoch [25/200], Loss: 0.0018\n",
      "Epoch [26/200], Loss: 0.0018\n",
      "Epoch [27/200], Loss: 0.0018\n",
      "Epoch [28/200], Loss: 0.0018\n",
      "Epoch [29/200], Loss: 0.0017\n",
      "Epoch [30/200], Loss: 0.0017\n",
      "Epoch [31/200], Loss: 0.0017\n",
      "Epoch [32/200], Loss: 0.0018\n",
      "Epoch [33/200], Loss: 0.0017\n",
      "Epoch [34/200], Loss: 0.0018\n",
      "Epoch [35/200], Loss: 0.0017\n",
      "Epoch [36/200], Loss: 0.0017\n",
      "Epoch [37/200], Loss: 0.0017\n",
      "Epoch [38/200], Loss: 0.0017\n",
      "Epoch [39/200], Loss: 0.0017\n",
      "Epoch [40/200], Loss: 0.0017\n",
      "Epoch [41/200], Loss: 0.0017\n",
      "Epoch [42/200], Loss: 0.0017\n",
      "Epoch [43/200], Loss: 0.0017\n",
      "Epoch [44/200], Loss: 0.0017\n",
      "Epoch [45/200], Loss: 0.0017\n",
      "Epoch [46/200], Loss: 0.0017\n",
      "Epoch [47/200], Loss: 0.0017\n",
      "Epoch [48/200], Loss: 0.0017\n",
      "Epoch [49/200], Loss: 0.0017\n",
      "Epoch [50/200], Loss: 0.0016\n",
      "Epoch [51/200], Loss: 0.0017\n",
      "Epoch [52/200], Loss: 0.0017\n",
      "Epoch [53/200], Loss: 0.0017\n",
      "Epoch [54/200], Loss: 0.0017\n",
      "Epoch [55/200], Loss: 0.0017\n",
      "Epoch [56/200], Loss: 0.0016\n",
      "Epoch [57/200], Loss: 0.0017\n",
      "Epoch [58/200], Loss: 0.0017\n",
      "Epoch [59/200], Loss: 0.0016\n",
      "Epoch [60/200], Loss: 0.0016\n",
      "Epoch [61/200], Loss: 0.0016\n",
      "Epoch [62/200], Loss: 0.0016\n",
      "Epoch [63/200], Loss: 0.0016\n",
      "Epoch [64/200], Loss: 0.0017\n",
      "Epoch [65/200], Loss: 0.0016\n",
      "Epoch [66/200], Loss: 0.0016\n",
      "Epoch [67/200], Loss: 0.0016\n",
      "Epoch [68/200], Loss: 0.0017\n",
      "Epoch [69/200], Loss: 0.0016\n",
      "Epoch [70/200], Loss: 0.0016\n",
      "Epoch [71/200], Loss: 0.0016\n",
      "Epoch [72/200], Loss: 0.0017\n",
      "Epoch [73/200], Loss: 0.0016\n",
      "Epoch [74/200], Loss: 0.0016\n",
      "Epoch [75/200], Loss: 0.0016\n",
      "Epoch [76/200], Loss: 0.0016\n",
      "Epoch [77/200], Loss: 0.0016\n",
      "Epoch [78/200], Loss: 0.0016\n",
      "Epoch [79/200], Loss: 0.0017\n",
      "Epoch [80/200], Loss: 0.0016\n",
      "Epoch [81/200], Loss: 0.0016\n",
      "Epoch [82/200], Loss: 0.0016\n",
      "Epoch [83/200], Loss: 0.0016\n",
      "Epoch [84/200], Loss: 0.0016\n",
      "Epoch [85/200], Loss: 0.0016\n",
      "Epoch [86/200], Loss: 0.0016\n",
      "Epoch [87/200], Loss: 0.0016\n",
      "Epoch [88/200], Loss: 0.0016\n",
      "Epoch [89/200], Loss: 0.0016\n",
      "Epoch [90/200], Loss: 0.0016\n",
      "Epoch [91/200], Loss: 0.0016\n",
      "Epoch [92/200], Loss: 0.0016\n",
      "Epoch [93/200], Loss: 0.0016\n",
      "Epoch [94/200], Loss: 0.0016\n",
      "Epoch [95/200], Loss: 0.0016\n",
      "Epoch [96/200], Loss: 0.0016\n",
      "Epoch [97/200], Loss: 0.0016\n",
      "Epoch [98/200], Loss: 0.0016\n",
      "Epoch [99/200], Loss: 0.0016\n",
      "Epoch [100/200], Loss: 0.0016\n",
      "Epoch [101/200], Loss: 0.0016\n",
      "Epoch [102/200], Loss: 0.0016\n",
      "Epoch [103/200], Loss: 0.0016\n",
      "Epoch [104/200], Loss: 0.0016\n",
      "Epoch [105/200], Loss: 0.0016\n",
      "Epoch [106/200], Loss: 0.0016\n",
      "Epoch [107/200], Loss: 0.0016\n",
      "Epoch [108/200], Loss: 0.0016\n",
      "Epoch [109/200], Loss: 0.0016\n",
      "Epoch [110/200], Loss: 0.0016\n",
      "Epoch [111/200], Loss: 0.0016\n",
      "Epoch [112/200], Loss: 0.0016\n",
      "Epoch [113/200], Loss: 0.0016\n",
      "Epoch [114/200], Loss: 0.0016\n",
      "Epoch [115/200], Loss: 0.0016\n",
      "Epoch [116/200], Loss: 0.0016\n",
      "Epoch [117/200], Loss: 0.0016\n",
      "Epoch [118/200], Loss: 0.0016\n",
      "Epoch [119/200], Loss: 0.0016\n",
      "Epoch [120/200], Loss: 0.0016\n",
      "Epoch [121/200], Loss: 0.0016\n",
      "Epoch [122/200], Loss: 0.0016\n",
      "Epoch [123/200], Loss: 0.0016\n",
      "Epoch [124/200], Loss: 0.0016\n",
      "Epoch [125/200], Loss: 0.0016\n",
      "Epoch [126/200], Loss: 0.0016\n",
      "Epoch [127/200], Loss: 0.0016\n",
      "Epoch [128/200], Loss: 0.0016\n",
      "Epoch [129/200], Loss: 0.0016\n",
      "Epoch [130/200], Loss: 0.0016\n",
      "Epoch [131/200], Loss: 0.0016\n",
      "Epoch [132/200], Loss: 0.0016\n",
      "Epoch [133/200], Loss: 0.0016\n",
      "Epoch [134/200], Loss: 0.0016\n",
      "Epoch [135/200], Loss: 0.0016\n",
      "Epoch [136/200], Loss: 0.0016\n",
      "Epoch [137/200], Loss: 0.0016\n",
      "Epoch [138/200], Loss: 0.0016\n",
      "Epoch [139/200], Loss: 0.0016\n",
      "Epoch [140/200], Loss: 0.0016\n",
      "Epoch [141/200], Loss: 0.0016\n",
      "Epoch [142/200], Loss: 0.0016\n",
      "Epoch [143/200], Loss: 0.0016\n",
      "Epoch [144/200], Loss: 0.0016\n",
      "Epoch [145/200], Loss: 0.0016\n",
      "Epoch [146/200], Loss: 0.0016\n",
      "Epoch [147/200], Loss: 0.0016\n",
      "Epoch [148/200], Loss: 0.0016\n",
      "Epoch [149/200], Loss: 0.0016\n",
      "Epoch [150/200], Loss: 0.0016\n",
      "Epoch [151/200], Loss: 0.0016\n",
      "Epoch [152/200], Loss: 0.0016\n",
      "Epoch [153/200], Loss: 0.0016\n",
      "Epoch [154/200], Loss: 0.0016\n",
      "Epoch [155/200], Loss: 0.0016\n",
      "Epoch [156/200], Loss: 0.0016\n",
      "Epoch [157/200], Loss: 0.0016\n",
      "Epoch [158/200], Loss: 0.0016\n",
      "Epoch [159/200], Loss: 0.0016\n",
      "Epoch [160/200], Loss: 0.0015\n",
      "Epoch [161/200], Loss: 0.0016\n",
      "Epoch [162/200], Loss: 0.0016\n",
      "Epoch [163/200], Loss: 0.0016\n",
      "Epoch [164/200], Loss: 0.0016\n",
      "Epoch [165/200], Loss: 0.0016\n",
      "Epoch [166/200], Loss: 0.0016\n",
      "Epoch [167/200], Loss: 0.0016\n",
      "Epoch [168/200], Loss: 0.0016\n",
      "Epoch [169/200], Loss: 0.0016\n",
      "Epoch [170/200], Loss: 0.0016\n",
      "Epoch [171/200], Loss: 0.0016\n",
      "Epoch [172/200], Loss: 0.0016\n",
      "Epoch [173/200], Loss: 0.0015\n",
      "Epoch [174/200], Loss: 0.0016\n",
      "Epoch [175/200], Loss: 0.0016\n",
      "Epoch [176/200], Loss: 0.0016\n",
      "Epoch [177/200], Loss: 0.0016\n",
      "Epoch [178/200], Loss: 0.0016\n",
      "Epoch [179/200], Loss: 0.0016\n",
      "Epoch [180/200], Loss: 0.0016\n",
      "Epoch [181/200], Loss: 0.0016\n",
      "Epoch [182/200], Loss: 0.0016\n",
      "Epoch [183/200], Loss: 0.0016\n",
      "Epoch [184/200], Loss: 0.0016\n",
      "Epoch [185/200], Loss: 0.0016\n",
      "Epoch [186/200], Loss: 0.0016\n",
      "Epoch [187/200], Loss: 0.0015\n",
      "Epoch [188/200], Loss: 0.0016\n",
      "Epoch [189/200], Loss: 0.0016\n",
      "Epoch [190/200], Loss: 0.0016\n",
      "Epoch [191/200], Loss: 0.0015\n",
      "Epoch [192/200], Loss: 0.0016\n",
      "Epoch [193/200], Loss: 0.0016\n",
      "Epoch [194/200], Loss: 0.0016\n",
      "Epoch [195/200], Loss: 0.0016\n",
      "Epoch [196/200], Loss: 0.0015\n",
      "Epoch [197/200], Loss: 0.0016\n",
      "Epoch [198/200], Loss: 0.0016\n",
      "Epoch [199/200], Loss: 0.0016\n",
      "Epoch [200/200], Loss: 0.0016\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training Loop\n",
    "num_epochs = 200\n",
    "best_loss = float('inf')  # Initialize with a very high value\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data,) in enumerate(dataloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Check if this is the best model so far\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            # Save the model state\n",
    "            torch.save(model.state_dict(), 'trained_SimpleAutoencoder_best+arxiv.pth')\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_dataConv(model, numpy_arrays):\n",
    "#     encoded_tensors = []  # To collect the encoded tensors\n",
    "    \n",
    "#     for arr in numpy_arrays:\n",
    "#         # Check if the array can be reshaped to (1, 4, 384)\n",
    "#         if arr.size == 1536:\n",
    "#             reshaped_array = arr.reshape(1, 4, 384)\n",
    "            \n",
    "#             # Convert NumPy array to PyTorch tensor\n",
    "#             input_tensor = torch.tensor(reshaped_array, dtype=torch.float32).to(device)\n",
    "            \n",
    "#             with torch.no_grad():  # Disable gradient computation\n",
    "#                 encoded_tensor = model.encoder(input_tensor)  # Use only the encoder part\n",
    "#                 encoded_tensors.append(encoded_tensor.cpu())  # Move tensor back to CPU\n",
    "#         else:\n",
    "#             print(f\"Skipping array of shape {arr.shape}. Cannot be reshaped to (1, 4, 384).\")\n",
    "    \n",
    "#     return encoded_tensors  # Return the list of all encoded tensors\n",
    "# random_array = np.random.randn(256, 4, 384)\n",
    "\n",
    "# encoded_tensors = encode_dataConv(model, random_array)\n",
    "# print(len(encoded_tensors))\n",
    "# encoded_tensors[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gensim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
